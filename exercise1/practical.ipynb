{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline CNN with stochastic gradient descent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, no_grad\n",
    "from torch.nn import Conv2d, CrossEntropyLoss, LeakyReLU, Linear, MaxPool2d\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "cifar_10_training_data = CIFAR10('datasets/', download=True, transform=transform)\n",
    "cifar_10_test_data = CIFAR10('datasets/', train=False, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(cifar_10_training_data, batch_size=4, num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(cifar_10_test_data, batch_size=4, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "num_input_channels = 3\n",
    "num_output_classes = 10\n",
    "\n",
    "num_conv1_channels = 6\n",
    "conv_kernel_size = 5\n",
    "pool_kernel_size = 2\n",
    "num_conv2_channels = 16\n",
    "\n",
    "fc1_output_size = 120\n",
    "fc2_output_size = 84\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation=LeakyReLU, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Conv2d(\n",
    "            num_input_channels, num_conv1_channels, conv_kernel_size)\n",
    "        self.pool1 = MaxPool2d(pool_kernel_size, pool_kernel_size)\n",
    "        self.conv2 = Conv2d(\n",
    "            num_conv1_channels, num_conv2_channels, conv_kernel_size)\n",
    "        self.pool2 = MaxPool2d(pool_kernel_size, pool_kernel_size)\n",
    "        self.convolution_output_size = num_conv2_channels * conv_kernel_size**2\n",
    "        # Fully connected layers\n",
    "        self.fc1 = Linear(\n",
    "            num_conv2_channels * conv_kernel_size * conv_kernel_size, fc1_output_size)\n",
    "        self.fc2 = Linear(fc1_output_size, fc2_output_size)\n",
    "        self.fc3 = Linear(fc2_output_size, num_output_classes)\n",
    "        self.relu = activation(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        x = self.pool2(self.relu(self.conv2(x)))\n",
    "        # Flatten the output of the convolutional layers\n",
    "        x = x.view(-1, self.convolution_output_size)\n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def train_model(data_loader, network, optimizer, loss_function):\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(data_loader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = network(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % BATCH_TO_PRINT == BATCH_TO_PRINT - 1:\n",
    "                print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    return network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "net = Net(negative_slope=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.305\n",
      "[1,  4000] loss: 2.305\n",
      "[1,  6000] loss: 2.305\n",
      "[1,  8000] loss: 2.305\n",
      "[1, 10000] loss: 2.305\n",
      "[1, 12000] loss: 2.304\n",
      "[2,  2000] loss: 2.303\n",
      "[2,  4000] loss: 2.303\n",
      "[2,  6000] loss: 2.303\n",
      "[2,  8000] loss: 2.303\n",
      "[2, 10000] loss: 2.303\n",
      "[2, 12000] loss: 2.302\n",
      "[3,  2000] loss: 2.301\n",
      "[3,  4000] loss: 2.302\n",
      "[3,  6000] loss: 2.301\n",
      "[3,  8000] loss: 2.301\n",
      "[3, 10000] loss: 2.301\n",
      "[3, 12000] loss: 2.300\n",
      "[4,  2000] loss: 2.300\n",
      "[4,  4000] loss: 2.300\n",
      "[4,  6000] loss: 2.299\n",
      "[4,  8000] loss: 2.299\n",
      "[4, 10000] loss: 2.299\n",
      "[4, 12000] loss: 2.298\n",
      "[5,  2000] loss: 2.298\n",
      "[5,  4000] loss: 2.298\n",
      "[5,  6000] loss: 2.297\n",
      "[5,  8000] loss: 2.297\n",
      "[5, 10000] loss: 2.297\n",
      "[5, 12000] loss: 2.296\n",
      "[6,  2000] loss: 2.295\n",
      "[6,  4000] loss: 2.296\n",
      "[6,  6000] loss: 2.294\n",
      "[6,  8000] loss: 2.294\n",
      "[6, 10000] loss: 2.294\n",
      "[6, 12000] loss: 2.293\n",
      "[7,  2000] loss: 2.292\n",
      "[7,  4000] loss: 2.292\n",
      "[7,  6000] loss: 2.290\n",
      "[7,  8000] loss: 2.289\n",
      "[7, 10000] loss: 2.289\n",
      "[7, 12000] loss: 2.287\n",
      "[8,  2000] loss: 2.286\n",
      "[8,  4000] loss: 2.286\n",
      "[8,  6000] loss: 2.283\n",
      "[8,  8000] loss: 2.281\n",
      "[8, 10000] loss: 2.281\n",
      "[8, 12000] loss: 2.278\n",
      "[9,  2000] loss: 2.275\n",
      "[9,  4000] loss: 2.275\n",
      "[9,  6000] loss: 2.270\n",
      "[9,  8000] loss: 2.267\n",
      "[9, 10000] loss: 2.265\n",
      "[9, 12000] loss: 2.260\n",
      "[10,  2000] loss: 2.253\n",
      "[10,  4000] loss: 2.251\n",
      "[10,  6000] loss: 2.241\n",
      "[10,  8000] loss: 2.234\n",
      "[10, 10000] loss: 2.227\n",
      "[10, 12000] loss: 2.215\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_EPOCHS = 10\n",
    "BATCH_TO_PRINT = 2000\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the network\n",
    "trained_net = train_model(train_loader, net, optimizer, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate test accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def calculate_test_accuracy(test_loader, network):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = network(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 18.36 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_test_accuracy(test_loader, net)\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * accuracy} %')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The test accuracy is very low, most likely the learning rate is too low since the cross-entropy loss is decreasing very slowly over the epochs. It decreased the same amount during the 10th epoch as the previous nine combined."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Swapping the optimizer for ADAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.806\n",
      "[1,  4000] loss: 1.562\n",
      "[1,  6000] loss: 1.432\n",
      "[1,  8000] loss: 1.373\n",
      "[1, 10000] loss: 1.371\n",
      "[1, 12000] loss: 1.319\n",
      "[2,  2000] loss: 1.271\n",
      "[2,  4000] loss: 1.247\n",
      "[2,  6000] loss: 1.193\n",
      "[2,  8000] loss: 1.172\n",
      "[2, 10000] loss: 1.196\n",
      "[2, 12000] loss: 1.158\n",
      "[3,  2000] loss: 1.132\n",
      "[3,  4000] loss: 1.120\n",
      "[3,  6000] loss: 1.080\n",
      "[3,  8000] loss: 1.061\n",
      "[3, 10000] loss: 1.094\n",
      "[3, 12000] loss: 1.056\n",
      "[4,  2000] loss: 1.044\n",
      "[4,  4000] loss: 1.023\n",
      "[4,  6000] loss: 0.994\n",
      "[4,  8000] loss: 0.978\n",
      "[4, 10000] loss: 1.015\n",
      "[4, 12000] loss: 0.989\n",
      "[5,  2000] loss: 0.970\n",
      "[5,  4000] loss: 0.954\n",
      "[5,  6000] loss: 0.947\n",
      "[5,  8000] loss: 0.906\n",
      "[5, 10000] loss: 0.959\n",
      "[5, 12000] loss: 0.931\n",
      "[6,  2000] loss: 0.915\n",
      "[6,  4000] loss: 0.909\n",
      "[6,  6000] loss: 0.891\n",
      "[6,  8000] loss: 0.855\n",
      "[6, 10000] loss: 0.916\n",
      "[6, 12000] loss: 0.884\n",
      "[7,  2000] loss: 0.879\n",
      "[7,  4000] loss: 0.870\n",
      "[7,  6000] loss: 0.846\n",
      "[7,  8000] loss: 0.810\n",
      "[7, 10000] loss: 0.872\n",
      "[7, 12000] loss: 0.842\n",
      "[8,  2000] loss: 0.841\n",
      "[8,  4000] loss: 0.827\n",
      "[8,  6000] loss: 0.807\n",
      "[8,  8000] loss: 0.788\n",
      "[8, 10000] loss: 0.833\n",
      "[8, 12000] loss: 0.815\n",
      "[9,  2000] loss: 0.799\n",
      "[9,  4000] loss: 0.801\n",
      "[9,  6000] loss: 0.776\n",
      "[9,  8000] loss: 0.757\n",
      "[9, 10000] loss: 0.798\n",
      "[9, 12000] loss: 0.791\n",
      "[10,  2000] loss: 0.775\n",
      "[10,  4000] loss: 0.766\n",
      "[10,  6000] loss: 0.745\n",
      "[10,  8000] loss: 0.739\n",
      "[10, 10000] loss: 0.781\n",
      "[10, 12000] loss: 0.759\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "new_network = Net()\n",
    "adam_optimizer = Adam(new_network.parameters())\n",
    "\n",
    "trained_model_with_adam = train_model(train_loader, new_network, adam_optimizer, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 60.07%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_test_accuracy(test_loader, new_network)\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * accuracy}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracy is still not very high, but significantly better by only changing the optimizer method from stochastic gradient descent to ADAM."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Swapping the activation function for tanh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from torch.nn import Tanh\n",
    "\n",
    "\n",
    "network_with_tanh = Net(Tanh)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.816\n",
      "[1,  4000] loss: 1.591\n",
      "[1,  6000] loss: 1.477\n",
      "[1,  8000] loss: 1.401\n",
      "[1, 10000] loss: 1.416\n",
      "[1, 12000] loss: 1.350\n",
      "[2,  2000] loss: 1.316\n",
      "[2,  4000] loss: 1.310\n",
      "[2,  6000] loss: 1.254\n",
      "[2,  8000] loss: 1.231\n",
      "[2, 10000] loss: 1.259\n",
      "[2, 12000] loss: 1.220\n",
      "[3,  2000] loss: 1.210\n",
      "[3,  4000] loss: 1.218\n",
      "[3,  6000] loss: 1.160\n",
      "[3,  8000] loss: 1.155\n",
      "[3, 10000] loss: 1.194\n",
      "[3, 12000] loss: 1.156\n",
      "[4,  2000] loss: 1.151\n",
      "[4,  4000] loss: 1.168\n",
      "[4,  6000] loss: 1.126\n",
      "[4,  8000] loss: 1.115\n",
      "[4, 10000] loss: 1.145\n",
      "[4, 12000] loss: 1.116\n",
      "[5,  2000] loss: 1.119\n",
      "[5,  4000] loss: 1.150\n",
      "[5,  6000] loss: 1.093\n",
      "[5,  8000] loss: 1.094\n",
      "[5, 10000] loss: 1.117\n",
      "[5, 12000] loss: 1.088\n",
      "[6,  2000] loss: 1.106\n",
      "[6,  4000] loss: 1.127\n",
      "[6,  6000] loss: 1.058\n",
      "[6,  8000] loss: 1.061\n",
      "[6, 10000] loss: 1.108\n",
      "[6, 12000] loss: 1.075\n",
      "[7,  2000] loss: 1.079\n",
      "[7,  4000] loss: 1.084\n",
      "[7,  6000] loss: 1.048\n",
      "[7,  8000] loss: 1.045\n",
      "[7, 10000] loss: 1.093\n",
      "[7, 12000] loss: 1.061\n",
      "[8,  2000] loss: 1.070\n",
      "[8,  4000] loss: 1.088\n",
      "[8,  6000] loss: 1.021\n",
      "[8,  8000] loss: 1.026\n",
      "[8, 10000] loss: 1.071\n",
      "[8, 12000] loss: 1.052\n",
      "[9,  2000] loss: 1.046\n",
      "[9,  4000] loss: 1.060\n",
      "[9,  6000] loss: 1.024\n",
      "[9,  8000] loss: 1.026\n",
      "[9, 10000] loss: 1.064\n",
      "[9, 12000] loss: 1.041\n",
      "[10,  2000] loss: 1.052\n",
      "[10,  4000] loss: 1.052\n",
      "[10,  6000] loss: 1.028\n",
      "[10,  8000] loss: 1.011\n",
      "[10, 10000] loss: 1.065\n",
      "[10, 12000] loss: 1.026\n"
     ]
    },
    {
     "data": {
      "text/plain": "Net(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n  (relu): Tanh()\n)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam_optimizer = Adam(network_with_tanh.parameters())\n",
    "_ = train_model(train_loader, network_with_tanh, adam_optimizer, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 57.510%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_test_accuracy(test_loader, network_with_tanh)\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * accuracy:.3f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracy is lower using the hyperbolical tangent function as activation function in the network, compared to using the leaky ReLU function as activation function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}