{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline CNN with stochastic gradient descent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, no_grad\n",
    "from torch.nn import Conv2d, CrossEntropyLoss, LeakyReLU, Linear, MaxPool2d\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "cifar_10_training_data = CIFAR10('datasets/', download=True, transform=transform)\n",
    "cifar_10_test_data = CIFAR10('datasets/', train=False, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(cifar_10_training_data, batch_size=4, num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(cifar_10_test_data, batch_size=4, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "num_input_channels = 3\n",
    "num_output_classes = 10\n",
    "\n",
    "num_conv1_channels = 6\n",
    "conv_kernel_size = 5\n",
    "pool_kernel_size = 2\n",
    "num_conv2_channels = 16\n",
    "\n",
    "fc1_output_size = 120\n",
    "fc2_output_size = 84\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation=LeakyReLU, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Conv2d(\n",
    "            num_input_channels, num_conv1_channels, conv_kernel_size)\n",
    "        self.pool1 = MaxPool2d(pool_kernel_size, pool_kernel_size)\n",
    "        self.conv2 = Conv2d(\n",
    "            num_conv1_channels, num_conv2_channels, conv_kernel_size)\n",
    "        self.pool2 = MaxPool2d(pool_kernel_size, pool_kernel_size)\n",
    "        self.convolution_output_size = num_conv2_channels * conv_kernel_size**2\n",
    "        # Fully connected layers\n",
    "        self.fc1 = Linear(\n",
    "            num_conv2_channels * conv_kernel_size * conv_kernel_size, fc1_output_size)\n",
    "        self.fc2 = Linear(fc1_output_size, fc2_output_size)\n",
    "        self.fc3 = Linear(fc2_output_size, num_output_classes)\n",
    "\n",
    "        self.name = kwargs.pop('name', '')\n",
    "        self.relu = activation(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        x = self.pool2(self.relu(self.conv2(x)))\n",
    "        # Flatten the output of the convolutional layers\n",
    "        x = x.view(-1, self.convolution_output_size)\n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train_model(data_loader, network, optimizer, loss_function):\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        running_loss = epoch_loss = 0.\n",
    "        for i, data in enumerate(data_loader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = network(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            if i % BATCH_TO_PRINT == BATCH_TO_PRINT - 1:\n",
    "                print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / BATCH_TO_PRINT:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        writer.add_scalar(f'Loss/train: {network.name}', epoch_loss / len(data_loader), epoch)\n",
    "        print(f\"[{epoch + 1}] loss: {epoch_loss / len(data_loader):.3f}\")\n",
    "\n",
    "    writer.flush()\n",
    "    return network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "net = Net(negative_slope=0.1, name='Baseline model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.306\n",
      "[1,  4000] loss: 2.305\n",
      "[1,  6000] loss: 2.303\n",
      "[1,  8000] loss: 2.302\n",
      "[1, 10000] loss: 2.304\n",
      "[1, 12000] loss: 2.303\n",
      "[1] loss: 2.304\n",
      "[2,  2000] loss: 2.304\n",
      "[2,  4000] loss: 2.303\n",
      "[2,  6000] loss: 2.302\n",
      "[2,  8000] loss: 2.301\n",
      "[2, 10000] loss: 2.302\n",
      "[2, 12000] loss: 2.301\n",
      "[2] loss: 2.302\n",
      "[3,  2000] loss: 2.302\n",
      "[3,  4000] loss: 2.301\n",
      "[3,  6000] loss: 2.299\n",
      "[3,  8000] loss: 2.299\n",
      "[3, 10000] loss: 2.300\n",
      "[3, 12000] loss: 2.298\n",
      "[3] loss: 2.300\n",
      "[4,  2000] loss: 2.299\n",
      "[4,  4000] loss: 2.298\n",
      "[4,  6000] loss: 2.296\n",
      "[4,  8000] loss: 2.296\n",
      "[4, 10000] loss: 2.297\n",
      "[4, 12000] loss: 2.295\n",
      "[4] loss: 2.297\n",
      "[5,  2000] loss: 2.294\n",
      "[5,  4000] loss: 2.293\n",
      "[5,  6000] loss: 2.291\n",
      "[5,  8000] loss: 2.290\n",
      "[5, 10000] loss: 2.291\n",
      "[5, 12000] loss: 2.288\n",
      "[5] loss: 2.291\n",
      "[6,  2000] loss: 2.286\n",
      "[6,  4000] loss: 2.284\n",
      "[6,  6000] loss: 2.281\n",
      "[6,  8000] loss: 2.279\n",
      "[6, 10000] loss: 2.279\n",
      "[6, 12000] loss: 2.274\n",
      "[6] loss: 2.280\n",
      "[7,  2000] loss: 2.270\n",
      "[7,  4000] loss: 2.268\n",
      "[7,  6000] loss: 2.262\n",
      "[7,  8000] loss: 2.260\n",
      "[7, 10000] loss: 2.258\n",
      "[7, 12000] loss: 2.250\n",
      "[7] loss: 2.260\n",
      "[8,  2000] loss: 2.241\n",
      "[8,  4000] loss: 2.239\n",
      "[8,  6000] loss: 2.229\n",
      "[8,  8000] loss: 2.226\n",
      "[8, 10000] loss: 2.221\n",
      "[8, 12000] loss: 2.208\n",
      "[8] loss: 2.226\n",
      "[9,  2000] loss: 2.195\n",
      "[9,  4000] loss: 2.194\n",
      "[9,  6000] loss: 2.176\n",
      "[9,  8000] loss: 2.176\n",
      "[9, 10000] loss: 2.169\n",
      "[9, 12000] loss: 2.154\n",
      "[9] loss: 2.176\n",
      "[10,  2000] loss: 2.143\n",
      "[10,  4000] loss: 2.147\n",
      "[10,  6000] loss: 2.130\n",
      "[10,  8000] loss: 2.133\n",
      "[10, 10000] loss: 2.129\n",
      "[10, 12000] loss: 2.116\n",
      "[10] loss: 2.132\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_EPOCHS = 10\n",
    "BATCH_TO_PRINT = 2000\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the network\n",
    "trained_net = train_model(train_loader, net, optimizer, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate test accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def calculate_test_accuracy(test_loader, network):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = network(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 24.37 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_test_accuracy(test_loader, net)\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * accuracy} %')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The test accuracy is very low, most likely the learning rate is too low since the cross-entropy loss is decreasing very slowly over the epochs. It decreased the same amount during the 10th epoch as the previous nine combined."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Swapping the optimizer for ADAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.852\n",
      "[1,  4000] loss: 1.604\n",
      "[1,  6000] loss: 1.491\n",
      "[1,  8000] loss: 1.425\n",
      "[1, 10000] loss: 1.419\n",
      "[1, 12000] loss: 1.369\n",
      "[1] loss: 1.520\n",
      "[2,  2000] loss: 1.311\n",
      "[2,  4000] loss: 1.309\n",
      "[2,  6000] loss: 1.263\n",
      "[2,  8000] loss: 1.233\n",
      "[2, 10000] loss: 1.235\n",
      "[2, 12000] loss: 1.202\n",
      "[2] loss: 1.256\n",
      "[3,  2000] loss: 1.165\n",
      "[3,  4000] loss: 1.175\n",
      "[3,  6000] loss: 1.138\n",
      "[3,  8000] loss: 1.121\n",
      "[3, 10000] loss: 1.122\n",
      "[3, 12000] loss: 1.095\n",
      "[3] loss: 1.133\n",
      "[4,  2000] loss: 1.070\n",
      "[4,  4000] loss: 1.083\n",
      "[4,  6000] loss: 1.055\n",
      "[4,  8000] loss: 1.039\n",
      "[4, 10000] loss: 1.051\n",
      "[4, 12000] loss: 1.021\n",
      "[4] loss: 1.051\n",
      "[5,  2000] loss: 1.013\n",
      "[5,  4000] loss: 1.005\n",
      "[5,  6000] loss: 0.991\n",
      "[5,  8000] loss: 0.976\n",
      "[5, 10000] loss: 1.000\n",
      "[5, 12000] loss: 0.965\n",
      "[5] loss: 0.991\n",
      "[6,  2000] loss: 0.957\n",
      "[6,  4000] loss: 0.963\n",
      "[6,  6000] loss: 0.941\n",
      "[6,  8000] loss: 0.930\n",
      "[6, 10000] loss: 0.962\n",
      "[6, 12000] loss: 0.924\n",
      "[6] loss: 0.944\n",
      "[7,  2000] loss: 0.917\n",
      "[7,  4000] loss: 0.912\n",
      "[7,  6000] loss: 0.893\n",
      "[7,  8000] loss: 0.886\n",
      "[7, 10000] loss: 0.912\n",
      "[7, 12000] loss: 0.889\n",
      "[7] loss: 0.899\n",
      "[8,  2000] loss: 0.874\n",
      "[8,  4000] loss: 0.878\n",
      "[8,  6000] loss: 0.854\n",
      "[8,  8000] loss: 0.842\n",
      "[8, 10000] loss: 0.892\n",
      "[8, 12000] loss: 0.883\n",
      "[8] loss: 0.868\n",
      "[9,  2000] loss: 0.851\n",
      "[9,  4000] loss: 0.838\n",
      "[9,  6000] loss: 0.832\n",
      "[9,  8000] loss: 0.819\n",
      "[9, 10000] loss: 0.861\n",
      "[9, 12000] loss: 0.846\n",
      "[9] loss: 0.839\n",
      "[10,  2000] loss: 0.820\n",
      "[10,  4000] loss: 0.809\n",
      "[10,  6000] loss: 0.818\n",
      "[10,  8000] loss: 0.799\n",
      "[10, 10000] loss: 0.837\n",
      "[10, 12000] loss: 0.818\n",
      "[10] loss: 0.815\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "new_network = Net(name='Model used with ADAM')\n",
    "adam_optimizer = Adam(new_network.parameters())\n",
    "\n",
    "trained_model_with_adam = train_model(train_loader, new_network, adam_optimizer, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 60.699999999999996%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_test_accuracy(test_loader, new_network)\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * accuracy}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracy is still not very high, but significantly better by only changing the optimizer method from stochastic gradient descent to ADAM."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Swapping the activation function for tanh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.nn import Tanh\n",
    "\n",
    "\n",
    "network_with_tanh = Net(Tanh, name='Model with tanh/ADAM')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.801\n",
      "[1,  4000] loss: 1.609\n",
      "[1,  6000] loss: 1.478\n",
      "[1,  8000] loss: 1.407\n",
      "[1, 10000] loss: 1.403\n",
      "[1, 12000] loss: 1.361\n",
      "[1] loss: 1.502\n",
      "[2,  2000] loss: 1.315\n",
      "[2,  4000] loss: 1.312\n",
      "[2,  6000] loss: 1.259\n",
      "[2,  8000] loss: 1.239\n",
      "[2, 10000] loss: 1.279\n",
      "[2, 12000] loss: 1.242\n",
      "[2] loss: 1.272\n",
      "[3,  2000] loss: 1.211\n",
      "[3,  4000] loss: 1.224\n",
      "[3,  6000] loss: 1.184\n",
      "[3,  8000] loss: 1.181\n",
      "[3, 10000] loss: 1.217\n",
      "[3, 12000] loss: 1.178\n",
      "[3] loss: 1.198\n",
      "[4,  2000] loss: 1.154\n",
      "[4,  4000] loss: 1.182\n",
      "[4,  6000] loss: 1.140\n",
      "[4,  8000] loss: 1.133\n",
      "[4, 10000] loss: 1.171\n",
      "[4, 12000] loss: 1.143\n",
      "[4] loss: 1.153\n",
      "[5,  2000] loss: 1.135\n",
      "[5,  4000] loss: 1.139\n",
      "[5,  6000] loss: 1.102\n",
      "[5,  8000] loss: 1.104\n",
      "[5, 10000] loss: 1.148\n",
      "[5, 12000] loss: 1.113\n",
      "[5] loss: 1.123\n",
      "[6,  2000] loss: 1.110\n",
      "[6,  4000] loss: 1.130\n",
      "[6,  6000] loss: 1.091\n",
      "[6,  8000] loss: 1.088\n",
      "[6, 10000] loss: 1.113\n",
      "[6, 12000] loss: 1.085\n",
      "[6] loss: 1.102\n",
      "[7,  2000] loss: 1.079\n",
      "[7,  4000] loss: 1.104\n",
      "[7,  6000] loss: 1.073\n",
      "[7,  8000] loss: 1.067\n",
      "[7, 10000] loss: 1.102\n",
      "[7, 12000] loss: 1.068\n",
      "[7] loss: 1.080\n",
      "[8,  2000] loss: 1.062\n",
      "[8,  4000] loss: 1.084\n",
      "[8,  6000] loss: 1.061\n",
      "[8,  8000] loss: 1.051\n",
      "[8, 10000] loss: 1.092\n",
      "[8, 12000] loss: 1.059\n",
      "[8] loss: 1.067\n",
      "[9,  2000] loss: 1.052\n",
      "[9,  4000] loss: 1.075\n",
      "[9,  6000] loss: 1.030\n",
      "[9,  8000] loss: 1.047\n",
      "[9, 10000] loss: 1.080\n",
      "[9, 12000] loss: 1.051\n",
      "[9] loss: 1.055\n",
      "[10,  2000] loss: 1.038\n",
      "[10,  4000] loss: 1.071\n",
      "[10,  6000] loss: 1.026\n",
      "[10,  8000] loss: 1.020\n",
      "[10, 10000] loss: 1.084\n",
      "[10, 12000] loss: 1.038\n",
      "[10] loss: 1.045\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = Adam(network_with_tanh.parameters())\n",
    "_ = train_model(train_loader, network_with_tanh, adam_optimizer, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 55.810%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_test_accuracy(test_loader, network_with_tanh)\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * accuracy:.3f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracy is lower using the hyperbolical tangent function as activation function in the network, compared to using the leaky ReLU function as activation function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}