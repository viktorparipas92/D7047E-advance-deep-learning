{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac8fd8d",
   "metadata": {},
   "source": [
    "1) What are the advantages of t-SNE over PCA?\n",
    "\n",
    "t-SNE and PCA are both techniques used for dimensionality reduction, but they differ in their approaches and the results they provide. Some advantages of t-SNE over PCA include:\n",
    "\n",
    "•t-SNE is better at capturing non-linear relationships between data points, while PCA assumes that the relationships are linear.\n",
    "\n",
    "•t-SNE can preserve the local structure of the data, which is useful when analyzing complex datasets with many clusters or subgroups.\n",
    "\n",
    "•t-SNE produces a probability distribution that can be used to measure the similarity between data points, while PCA only produces a linear transformation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "191edcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a38ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pairwise_similarities(X, Y):\n",
    "    \"\"\"\n",
    "    Calculates the pairwise similarities for P and Q given the high-dimensional\n",
    "    and low-dimensional embeddings, respectively.\n",
    "    \"\"\"\n",
    "    # Compute pairwise distances\n",
    "    dists_X = squareform(pdist(X, 'euclidean'))\n",
    "    dists_Y = squareform(pdist(Y, 'euclidean'))\n",
    "    \n",
    "    # Compute Gaussian similarities for P\n",
    "    sigma_X = np.var(dists_X)\n",
    "    P = np.exp(-dists_X ** 2 / (2 * sigma_X))\n",
    "    np.fill_diagonal(P, 0)  # Set diagonal to 0\n",
    "    P /= np.sum(P)  # Normalize\n",
    "    \n",
    "#     Q = np.exp(-dists_Y ** 2 / (2 * (1/math.sqrt(2))))\n",
    "#     np.fill_diagonal(Q, 0)  # Set diagonal to 0\n",
    "#     Q /= np.sum(Q)  # Normalize   \n",
    "    \n",
    "    # Compute Student t-distribution similarities for Q\n",
    "    # Picked this due to crowding problem for Gaussian\n",
    "     \n",
    "    dof = 1  # Degrees of freedom for Student t-distribution\n",
    "    t = 1 / (1 + (dists_Y ** 2 / dof))\n",
    "    np.fill_diagonal(t, 0)  # Set diagonal to 0\n",
    "    Q = t / np.sum(t)  # Normalize\n",
    "    return P, Q\n",
    "\n",
    "\n",
    "def kl_divergence(P, Q, eps=1e-10):\n",
    "\n",
    "    Q = np.clip(Q, eps, None)  # Clip Q to avoid log(0)\n",
    "    P = np.clip(P, eps, None)  # Clip P to avoid log(0)\n",
    "    C = np.sum(P * np.log(P / Q))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ef7414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1 loss: 0.43315572944277536\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1: a, b, and c are all close to each other in the low-dimensional space, \n",
    "# but a and b are very close to each other in the high-dimensional space while c is far away\n",
    "\n",
    "X1 = np.array([[1, 2, 3], [2, 3, 4], [100, 120, 130]])\n",
    "Y1 = np.array([[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]])\n",
    "\n",
    "# Compute pairwise similarities\n",
    "P1,Q1  =pairwise_similarities (X1, Y1)\n",
    "loss1 = kl_divergence(P1, Q1)\n",
    "print(\"Scenario 1 loss:\", loss1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26b13f",
   "metadata": {},
   "source": [
    "In the first scenario where a, b, and c are all close to each other in the low-dimensional space, the pairwise similarities between them will be high, since they are close to each other. However, the pairwise similarities between a and b in the high-dimensional space were very high, while the similarity between c and either a or b was very low. As a result, the loss function will be high because there is a mismatch between the pairwise similarities in the high-dimensional space and the low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfe432c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 2 loss: 1.1037919886644614\n"
     ]
    }
   ],
   "source": [
    "# Scenario 2: a and b are close to each other in the low-dimensional space, but c is far away from them\n",
    "X2 = np.array([[1, 2, 3], [2, 3, 4], [100, 120, 130]])\n",
    "Y2 = np.array([[0.1, 0.2], [0.2, 0.3], [55,45]])\n",
    "\n",
    "# Compute pairwise similarities\n",
    "P2, Q2 = pairwise_similarities (X2, Y2)\n",
    "loss2 = kl_divergence(P2, Q2)\n",
    "print(\"Scenario 2 loss:\", loss2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cab3b7",
   "metadata": {},
   "source": [
    "The KL divergence is likely to be relatively low, indicating that the t-SNE algorithm is able to effectively represent the data in the low-dimensional space,since the pairwise similarities between a and b are high in both the high-dimensional and low-dimensional spaces, and the pairwise similarities between c and a (or c and b) are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0133a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 3 loss: 0.30040886086641033\n"
     ]
    }
   ],
   "source": [
    "# Scenario 3: a, b, and c are all far away from each other in the low-dimensional space\n",
    "X3 = np.array([[1, 2, 3], [2, 3, 4], [100, 120, 130]])\n",
    "Y3 = np.array([[0.1, 0.2], [500.9, 500.8], [1000.5, 1000.6]])\n",
    "\n",
    "P3, Q3 = pairwise_similarities (X3, Y3)\n",
    "loss3 = kl_divergence(P3, Q3)\n",
    "print(\"Scenario 3 loss:\", loss3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96cdc8",
   "metadata": {},
   "source": [
    "Since the pairwise similarities between a, b, and c are low in the low-dimensional space and the pairwise similarities between a and b are high in the high-dimensional space, the KL divergence is likely to be relatively high, indicating that the t-SNE algorithm may have difficulty representing the data in the low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42d99471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 4 loss: 6.5455777119206315\n"
     ]
    }
   ],
   "source": [
    "# Scenario 4: a is far away from both b and c, which are close to each other in the low-dimensional space\n",
    "X4 = np.array([[1, 2, 3], [2, 3, 4], [100, 120, 130]])\n",
    "Y4 = np.array([[0.1, 0.2], [58.8, 58.9], [58.9, 60.0]])\n",
    "\n",
    "P4, Q4 = pairwise_similarities (X4, Y4)\n",
    "loss4 = kl_divergence(P4, Q4)\n",
    "print(\"Scenario 4 loss:\", loss4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83b5db",
   "metadata": {},
   "source": [
    "Since the pairwise similarities between a, b, and c are low in the low-dimensional space and the pairwise similarities between b and c are high in the high-dimensional space, the KL divergence is likely to be relatively high, indicating that the t-SNE algorithm may have difficulty representing the data in the low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eec7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
